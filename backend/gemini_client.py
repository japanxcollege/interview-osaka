"""
Gemini API Client
AIè³ªå•ææ¡ˆã¨è¦ç´„ç”Ÿæˆ
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Optional
from dotenv import load_dotenv
import google.generativeai as genai
from models import Utterance

BASE_DIR = Path(__file__).resolve().parents[1]
load_dotenv()
load_dotenv(dotenv_path=BASE_DIR / ".env", override=False)
logger = logging.getLogger(__name__)


class GeminiClient:
    """Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            logger.warning("âš ï¸ GEMINI_API_KEY not set. AI features will be disabled.")
            self.enabled = False
            return

        try:
            genai.configure(api_key=self.api_key)
            
            # Safety settings to allow all content (since this is an editor for adults/interviews)
            from google.generativeai.types import HarmCategory, HarmBlockThreshold
            
            self.safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
            
            self.model = genai.GenerativeModel(
                'gemini-flash-latest',
                # safety_settings=self.safety_settings # Constructor might accept it or generate_content
            )
            self.enabled = True
            logger.info("âœ… Gemini API initialized with BLOCK_NONE safety settings")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini API: {e}")
            self.enabled = False

    async def suggest_question(
        self,
        front_summary: str,
        recent_transcript: List[Utterance],
        previous_questions: Optional[List[str]] = None
    ) -> Optional[str]:
        """
        ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®è³ªå•ã‚’ææ¡ˆ

        Args:
            front_summary: ã“ã‚Œã¾ã§ã®è¦ç´„
            recent_transcript: ç›´è¿‘ã®ç™ºè©±ãƒªã‚¹ãƒˆ
            previous_questions: æ—¢ã«ææ¡ˆã—ãŸè³ªå•ã®å±¥æ­´

        Returns:
            ææ¡ˆè³ªå• or None
        """
        if not self.enabled:
            return None

        try:
            # ç›´è¿‘ã®ç™ºè©±ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            recent_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in recent_transcript[-5:]  # æœ€æ–°5ç™ºè©±
            ])
            previous_text = "\n".join(
                f"- {q.strip()}"
                for q in (previous_questions or [])
                if q and q.strip()
            )

            prompt = f"""ã‚ãªãŸã¯ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹AIã§ã™ã€‚

# ã“ã‚Œã¾ã§ã®ä¼šè©±ã®è¦ç´„
{front_summary if front_summary else "ï¼ˆã¾ã è¦ç´„ãªã—ï¼‰"}

# ç›´è¿‘ã®ä¼šè©±
{recent_text}

# ã“ã‚Œã¾ã§ã«AIãŒææ¡ˆã—ãŸè³ªå•
{previous_text if previous_text else "ï¼ˆã¾ã ã‚ã‚Šã¾ã›ã‚“ï¼‰"}

ä¸Šè¨˜ã®ä¼šè©±ã‚’è¸ã¾ãˆã¦ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ãŒæ¬¡ã«å°‹ã­ã‚‹ã¹ãè³ªå•ã‚’1ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
è³ªå•ã¯å…·ä½“çš„ã§ã€ä¼šè©±ã‚’æ·±ã‚ã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚
- ã“ã‚Œã¾ã§ã«ææ¡ˆã—ãŸè³ªå•ã‚„ã€ãã‚Œã¨ã»ã¼åŒã˜è¶£æ—¨ã®è³ªå•ã¯é¿ã‘ã¦ãã ã•ã„ã€‚
- ã™ã§ã«å›ç­”ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚
- ä¼šè©±å†…å®¹ã«å³ã—ãŸè³ªå•ã«ã—ã¦ãã ã•ã„ï¼ˆæ±ç”¨çš„ãªã€Œã©ã®ã‚ˆã†ãªãŠè©±ã§ã™ã‹ï¼Ÿã€ãªã©ã¯ç¦æ­¢ï¼‰ã€‚

å›ç­”ã¯è³ªå•æ–‡ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ä¸è¦ï¼‰ã€‚
"""

            response = await self.model.generate_content_async(prompt, safety_settings=self.safety_settings)
            query_text = response.text.strip()

            logger.info(f"ğŸ’¡ Suggested question: {query_text[:50]}...")
            return query_text

        except Exception as e:
            logger.error(f"Failed to suggest question: {e}")
            return None

    async def summarize_transcript(
        self,
        utterances: List[Utterance]
    ) -> Optional[str]:
        """
        ç™ºè©±ãƒªã‚¹ãƒˆã‚’è¦ç´„

        Args:
            utterances: ç™ºè©±ãƒªã‚¹ãƒˆ

        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ or None
        """
        if not self.enabled:
            return None

        if not utterances:
            return ""

        try:
            # ç™ºè©±ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            transcript_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in utterances
            ])

            prompt = f"""ä»¥ä¸‹ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®ä¼šè©±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# ä¼šè©±å†…å®¹
{transcript_text}

è¦ç´„ã®ãƒ«ãƒ¼ãƒ«:
- 3-5æ–‡ç¨‹åº¦ã§ç°¡æ½”ã«
- è©±ã•ã‚ŒãŸä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã‚’å«ã‚ã‚‹
- è©±è€…åã‚’å«ã‚ã¦ã€Œã€‡ã€‡ã•ã‚“ã¯...ã€ã¨ã„ã†å½¢å¼ã§

è¦ç´„:
"""

            response = await self.model.generate_content_async(prompt)
            summary = response.text.strip()

            logger.info(f"ğŸ“ Generated summary: {len(summary)} chars")
            return summary

        except Exception as e:
            logger.error(f"Failed to summarize transcript: {e}")
            return None

    async def generate_final_summary(
        self,
        front_summary: str,
        recent_transcript: List[Utterance]
    ) -> Optional[str]:
        """
        æœ€çµ‚è¦ç´„ã‚’ç”Ÿæˆ

        Args:
            front_summary: å‰åŠã®è¦ç´„
            recent_transcript: ç›´è¿‘ã®ç™ºè©±

        Returns:
            æœ€çµ‚è¦ç´„ or None
        """
        if not self.enabled:
            return None

        try:
            recent_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in recent_transcript
            ])

            prompt = f"""ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å…¨ä½“ã®æœ€çµ‚è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# å‰åŠã®è¦ç´„
{front_summary if front_summary else "ï¼ˆãªã—ï¼‰"}

# å¾ŒåŠã®ä¼šè©±
{recent_text}

æœ€çµ‚è¦ç´„:
- 5-10æ–‡ç¨‹åº¦
- ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å…¨ä½“ã®æµã‚Œã‚’æŠŠæ¡ã§ãã‚‹ã‚ˆã†ã«
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç®‡æ¡æ›¸ãã§å«ã‚ã‚‹
"""

            response = await self.model.generate_content_async(prompt)
            summary = response.text.strip()

            logger.info(f"ğŸ“„ Generated final summary: {len(summary)} chars")
            return summary

        except Exception as e:
            logger.error(f"Failed to generate final summary: {e}")
            return None

    async def improve_selected_text(
        self,
        selected_text: str,
        instruction: str,
        context: str = ""
    ) -> Optional[str]:
        """
        é¸æŠç¯„å›²ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ”¹å–„

        Args:
            selected_text: é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            instruction: æ”¹å–„æŒ‡ç¤ºï¼ˆ"ãƒ–ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒƒãƒ—", "æ›¸ãç›´ã—", ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
            context: å‰å¾Œã®æ–‡è„ˆ

        Returns:
            æ”¹å–„ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ or None
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚

# é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
{selected_text}

# å‰å¾Œã®æ–‡è„ˆï¼ˆå‚è€ƒï¼‰
{context if context else "ï¼ˆæ–‡è„ˆãªã—ï¼‰"}

# æŒ‡ç¤º
{instruction}

# é‡è¦ãªæ³¨æ„äº‹é …
- é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®éƒ¨åˆ†ã ã‘ã‚’æ”¹å–„ã—ã¦ãã ã•ã„
- ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ï¼ˆ```markdown ãªã©ï¼‰ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„
- ç´”ç²‹ãªæ”¹å–„å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„

æ”¹å–„å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ:
"""

            response = await self.model.generate_content_async(prompt)
            improved = response.text.strip()

            # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã‚’å‰Šé™¤
            if improved.startswith('```markdown'):
                improved = improved[len('```markdown'):].strip()
            if improved.startswith('```'):
                improved = improved[3:].strip()
            if improved.endswith('```'):
                improved = improved[:-3].strip()

            logger.info(f"âœ¨ Improved text: {len(improved)} chars")
            return improved

        except Exception as e:
            logger.error(f"Failed to improve text: {e}")
            return None

    async def restructure_as_subsection(
        self,
        selected_text: str,
        full_article: str
    ) -> Optional[str]:
        """
        é¸æŠç¯„å›²ã‚’1ã¤ã®å°è¦‹å‡ºã—ï¼ˆ##ï¼‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å†æ§‹æˆ

        Args:
            selected_text: é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            full_article: è¨˜äº‹å…¨ä½“

        Returns:
            å†æ§‹æˆã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ## è¦‹å‡ºã—ä»˜ãï¼‰or None
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã€1ã¤ã®ã¾ã¨ã¾ã£ãŸå°è¦‹å‡ºã—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å†æ§‹æˆã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹å…¨ä½“ï¼ˆå‚è€ƒï¼‰
{full_article}

# é¸æŠç¯„å›²ï¼ˆã“ã®éƒ¨åˆ†ã‚’å†æ§‹æˆï¼‰
{selected_text}

# æŒ‡ç¤º
1. é¸æŠç¯„å›²ã®å†…å®¹ã‚’åˆ†æã—ã€é©åˆ‡ãªå°è¦‹å‡ºã—ï¼ˆ##ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
2. æ•£ã‚‰ã°ã£ãŸå†…å®¹ã‚’ã€1ã¤ã®ã¾ã¨ã¾ã£ãŸæ–‡ç« ã«å†æ§‹æˆã—ã¦ãã ã•ã„
3. è¨˜äº‹ã®ãƒˆãƒ¼ãƒ³ã«åˆã‚ã›ã¦ãã ã•ã„
4. Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„
5. ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ï¼ˆ```markdownï¼‰ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„

å†æ§‹æˆã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³:
"""

            response = await self.model.generate_content_async(prompt)
            section = response.text.strip()

            # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã‚’å‰Šé™¤
            if section.startswith('```markdown'):
                section = section[len('```markdown'):].strip()
            if section.startswith('```'):
                section = section[3:].strip()
            if section.endswith('```'):
                section = section[:-3].strip()

            logger.info(f"ğŸ“¦ Restructured subsection: {len(section)} chars")
            return section

        except Exception as e:
            logger.error(f"Failed to restructure subsection: {e}")
            return None

    async def restructure_as_section(
        self,
        selected_text: str,
        full_article: str
    ) -> Optional[str]:
        """
        é¸æŠç¯„å›²ã‚’1ã¤ã®å¤§è¦‹å‡ºã—ï¼ˆ#ï¼‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å†æ§‹æˆ

        Args:
            selected_text: é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            full_article: è¨˜äº‹å…¨ä½“

        Returns:
            å†æ§‹æˆã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ# è¦‹å‡ºã—ä»˜ãï¼‰or None
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã€1ã¤ã®ã¾ã¨ã¾ã£ãŸå¤§è¦‹å‡ºã—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å†æ§‹æˆã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹å…¨ä½“ï¼ˆå‚è€ƒï¼‰
{full_article}

# é¸æŠç¯„å›²ï¼ˆã“ã®éƒ¨åˆ†ã‚’å†æ§‹æˆï¼‰
{selected_text}

# æŒ‡ç¤º
1. é¸æŠç¯„å›²ã®å†…å®¹ã‚’åˆ†æã—ã€é©åˆ‡ãªå¤§è¦‹å‡ºã—ï¼ˆ#ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
2. ã‚ˆã‚Šå¤§ããªãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ã€è¤‡æ•°ã®å°è¦‹å‡ºã—ï¼ˆ##ï¼‰ã‚’å«ã‚€æ§‹é€ çš„ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å†æ§‹æˆã—ã¦ãã ã•ã„
3. æ•£ã‚‰ã°ã£ãŸå†…å®¹ã‚’ã€è«–ç†çš„ãªæµã‚Œã‚’æŒã¤ã¾ã¨ã¾ã£ãŸæ–‡ç« ã«å†æ§‹æˆã—ã¦ãã ã•ã„
4. è¨˜äº‹ã®ãƒˆãƒ¼ãƒ³ã«åˆã‚ã›ã¦ãã ã•ã„
5. Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„
6. ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ï¼ˆ```markdownï¼‰ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„

å†æ§‹æˆã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³:
"""

            response = await self.model.generate_content_async(prompt)
            section = response.text.strip()

            # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã‚’å‰Šé™¤
            if section.startswith('```markdown'):
                section = section[len('```markdown'):].strip()
            if section.startswith('```'):
                section = section[3:].strip()
            if section.endswith('```'):
                section = section[:-3].strip()

            logger.info(f"ğŸ“¦ Restructured major section: {len(section)} chars")
            return section

        except Exception as e:
            logger.error(f"Failed to restructure section: {e}")
            return None

    async def generate_article_section(
        self,
        current_article: str,
        recent_transcript: List[Utterance],
        front_summary: str = ""
    ) -> Optional[str]:
        """
        æ–‡å­—èµ·ã“ã—ã‹ã‚‰è¨˜äº‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆãƒ»è¿½è¨˜

        Args:
            current_article: ç¾åœ¨ã®åŸç¨¿å†…å®¹
            recent_transcript: ç›´è¿‘ã®ç™ºè©±ãƒªã‚¹ãƒˆï¼ˆæœ€æ–°5ç™ºè©±ï¼‰
            front_summary: ã“ã‚Œã¾ã§ã®è¦ç´„ï¼ˆæ–‡è„ˆè£œå¼·ç”¨ï¼‰

        Returns:
            è¿½åŠ ã™ã¹ãè¨˜äº‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ or None
        """
        if not self.enabled:
            return None

        try:
            # ç›´è¿‘ã®ç™ºè©±ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            recent_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in recent_transcript
            ])


            prompt = f"""ã‚ãªãŸã¯ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼è¨˜äº‹ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
æ–‡å­—èµ·ã“ã—ã‚’å…ƒã«ã€è¨˜äº‹ã®ä¸€éƒ¨ã‚’**Markdownå½¢å¼**ã§æ›¸ã„ã¦ãã ã•ã„ã€‚

# ç¾åœ¨ã®è¨˜äº‹å†…å®¹
{current_article if current_article else "ï¼ˆã¾ã è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚ï¼‰"}

# ã“ã‚Œã¾ã§ã®è¦ç´„ï¼ˆå‚è€ƒï¼‰
{front_summary if front_summary else "ï¼ˆè¦ç´„ãªã—ï¼‰"}

# ç›´è¿‘ã®ä¼šè©±ï¼ˆæ–‡å­—èµ·ã“ã—ï¼‰
{recent_text}

# æŒ‡ç¤º
1. **å¿…ãš `##` ã§å§‹ã¾ã‚‹å°è¦‹å‡ºã—ã‚’ä»˜ã‘ã¦ãã ã•ã„**ï¼ˆä¾‹: `## AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¯èƒ½æ€§`ï¼‰
2. å°è¦‹å‡ºã—ã®å¾Œã«ã€ä¼šè©±å†…å®¹ã‚’è‡ªç„¶ãªæ–‡ç« ã«å¤‰æ›ã—ãŸæœ¬æ–‡ã‚’æ›¸ã„ã¦ãã ã•ã„
3. ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¤ãƒ¼ã®ç™ºè¨€ã¯ãã®ã¾ã¾å¼•ç”¨ã—ã€è¨˜äº‹ã¨ã—ã¦èª­ã¿ã‚„ã™ãã—ã¦ãã ã•ã„
4. æ—¢å­˜ã®è¨˜äº‹å†…å®¹ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„
5. 150-300æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã—ã¦ãã ã•ã„
6. **çµ¶å¯¾ã«** ```markdown ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„
7. **çµ¶å¯¾ã«**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚„è©±è€…åï¼ˆ[19:23:55] Interviewer:ï¼‰ã‚’å«ã‚ãªã„ã§ãã ã•ã„

å‡ºåŠ›å½¢å¼ï¼ˆå¿…ãšå®ˆã£ã¦ãã ã•ã„ï¼‰:
## å°è¦‹å‡ºã—

æœ¬æ–‡ï¼ˆè‡ªç„¶ãªè¨˜äº‹æ–‡ä½“ã§ï¼‰
"""

            response = await self.model.generate_content_async(prompt)
            section = response.text.strip()

            # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã‚’å‰Šé™¤ï¼ˆå¿µã®ãŸã‚ï¼‰
            if section.startswith('```markdown'):
                section = section[len('```markdown'):].strip()
            if section.startswith('```'):
                section = section[3:].strip()
            if section.endswith('```'):
                section = section[:-3].strip()

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãç™ºè©±è¡Œã‚’å‰Šé™¤ï¼ˆ[HH:MM:SS] Speaker:ï¼‰
            lines = section.split('\n')
            cleaned_lines = []
            for line in lines:
                # [19:23:55] Interviewer: ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if re.match(r'^\[\d{2}:\d{2}:\d{2}\]\s+\w+:\s*', line):
                    continue
                cleaned_lines.append(line)
            section = '\n'.join(cleaned_lines).strip()

            logger.info(f"ğŸ“ Generated article section: {len(section)} chars")
            return section

        except Exception as e:
            logger.error(f"Failed to generate article section: {e}")
            return None

    async def generate_text(self, system_prompt: str, user_prompt: str) -> Optional[str]:
        """
        æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå½¹å‰²ã‚„æŒ‡ç¤ºï¼‰
            user_prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã‚„å…·ä½“çš„ãªæŒ‡ç¤ºï¼‰
        """
        if not self.enabled:
            return None

        try:
            full_prompt = f"{system_prompt}\n\n---\n\n{user_prompt}"
            response = await self.model.generate_content_async(full_prompt, safety_settings=self.safety_settings)
            if response.prompt_feedback and response.prompt_feedback.block_reason:
                logger.warning(f"âš ï¸ Prompt blocked: {response.prompt_feedback}")
                return None
            return response.text.strip()
        except ValueError as e:
            # Often blocked content raises ValueError on .text access
            logger.warning(f"âš ï¸ Failed to get text from response (likely blocked): {e}")
            if response and response.prompt_feedback:
                 logger.warning(f"Prompt feedback: {response.prompt_feedback}")
            return None
        except Exception as e:
            logger.error(f"Failed to generate text (Gemini): {e}")
            return None


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
gemini_client = GeminiClient()
