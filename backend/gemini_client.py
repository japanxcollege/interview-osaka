"""
Gemini API Client
AIË≥™ÂïèÊèêÊ°à„Å®Ë¶ÅÁ¥ÑÁîüÊàê
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Optional
from dotenv import load_dotenv
import google.generativeai as genai
from models import Utterance

BASE_DIR = Path(__file__).resolve().parents[1]
load_dotenv()
load_dotenv(dotenv_path=BASE_DIR / ".env", override=False)
load_dotenv(dotenv_path=BASE_DIR / "backend" / ".env", override=True) # Backend specific env wins
logger = logging.getLogger(__name__)


class GeminiClient:
    """Gemini API„ÇØ„É©„Ç§„Ç¢„É≥„Éà"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            logger.warning("‚ö†Ô∏è GEMINI_API_KEY not set. AI features will be disabled.")
            self.enabled = False
            return

        try:
            genai.configure(api_key=self.api_key)
            
            # Safety settings to allow all content (since this is an editor for adults/interviews)
            from google.generativeai.types import HarmCategory, HarmBlockThreshold
            
            self.safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
            
            self.model = genai.GenerativeModel(
                'gemini-flash-latest',
                # safety_settings=self.safety_settings # Constructor might accept it or generate_content
            )
            self.enabled = True
            logger.info("‚úÖ Gemini API initialized with BLOCK_NONE safety settings")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini API: {e}")
            self.enabled = False

    async def suggest_question(
        self,
        front_summary: str,
        recent_transcript: List[Utterance],
        previous_questions: Optional[List[str]] = None
    ) -> Optional[str]:
        """
        „Ç§„É≥„Çø„Éì„É•„Éº„ÅÆË≥™Âïè„ÇíÊèêÊ°à

        Args:
            front_summary: „Åì„Çå„Åæ„Åß„ÅÆË¶ÅÁ¥Ñ
            recent_transcript: Áõ¥Ëøë„ÅÆÁô∫Ë©±„É™„Çπ„Éà
            previous_questions: Êó¢„Å´ÊèêÊ°à„Åó„ÅüË≥™Âïè„ÅÆÂ±•Ê≠¥

        Returns:
            ÊèêÊ°àË≥™Âïè or None
        """
        if not self.enabled:
            return None

        try:
            # Áõ¥Ëøë„ÅÆÁô∫Ë©±„Çí„ÉÜ„Ç≠„Çπ„ÉàÂåñ
            recent_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in recent_transcript[-5:]  # ÊúÄÊñ∞5Áô∫Ë©±
            ])
            previous_text = "\n".join(
                f"- {q.strip()}"
                for q in (previous_questions or [])
                if q and q.strip()
            )

            prompt = f"""„ÅÇ„Å™„Åü„ÅØ„Ç§„É≥„Çø„Éì„É•„Ç¢„Éº„Çí„Çµ„Éù„Éº„Éà„Åô„ÇãAI„Åß„Åô„ÄÇ

# „Åì„Çå„Åæ„Åß„ÅÆ‰ºöË©±„ÅÆË¶ÅÁ¥Ñ
{front_summary if front_summary else "Ôºà„Åæ„Å†Ë¶ÅÁ¥Ñ„Å™„ÅóÔºâ"}

# Áõ¥Ëøë„ÅÆ‰ºöË©±
{recent_text}

# „Åì„Çå„Åæ„Åß„Å´AI„ÅåÊèêÊ°à„Åó„ÅüË≥™Âïè
{previous_text if previous_text else "Ôºà„Åæ„Å†„ÅÇ„Çä„Åæ„Åõ„ÇìÔºâ"}

‰∏äË®ò„ÅÆ‰ºöË©±„ÇíË∏è„Åæ„Åà„Å¶„ÄÅ„Ç§„É≥„Çø„Éì„É•„Ç¢„Éº„ÅåÊ¨°„Å´Â∞ã„Å≠„Çã„Åπ„ÅçË≥™Âïè„Çí1„Å§ÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Ë≥™Âïè„ÅØÂÖ∑‰ΩìÁöÑ„Åß„ÄÅ‰ºöË©±„ÇíÊ∑±„ÇÅ„Çã„ÇÇ„ÅÆ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- „Åì„Çå„Åæ„Åß„Å´ÊèêÊ°à„Åó„ÅüË≥™Âïè„ÇÑ„ÄÅ„Åù„Çå„Å®„Åª„ÅºÂêå„ÅòË∂£Êó®„ÅÆË≥™Âïè„ÅØÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- „Åô„Åß„Å´ÂõûÁ≠î„Åï„Çå„Å¶„ÅÑ„ÇãÂÜÖÂÆπ„ÇíÁπ∞„ÇäËøî„Åï„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
- ‰ºöË©±ÂÜÖÂÆπ„Å´Âç≥„Åó„ÅüË≥™Âïè„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÊ±éÁî®ÁöÑ„Å™„Äå„Å©„ÅÆ„Çà„ÅÜ„Å™„ÅäË©±„Åß„Åô„ÅãÔºü„Äç„Å™„Å©„ÅØÁ¶ÅÊ≠¢Ôºâ„ÄÇ

ÂõûÁ≠î„ÅØË≥™ÂïèÊñá„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàË™¨Êòé‰∏çË¶ÅÔºâ„ÄÇ
"""

            response = await self.model.generate_content_async(prompt, safety_settings=self.safety_settings)
            query_text = response.text.strip()

            logger.info(f"üí° Suggested question: {query_text[:50]}...")
            return query_text

        except Exception as e:
            logger.error(f"Failed to suggest question: {e}")
            return None

    async def summarize_transcript(
        self,
        utterances: List[Utterance]
    ) -> Optional[str]:
        """
        Áô∫Ë©±„É™„Çπ„Éà„ÇíË¶ÅÁ¥Ñ

        Args:
            utterances: Áô∫Ë©±„É™„Çπ„Éà

        Returns:
            Ë¶ÅÁ¥Ñ„ÉÜ„Ç≠„Çπ„Éà or None
        """
        if not self.enabled:
            return None

        if not utterances:
            return ""

        try:
            # Áô∫Ë©±„Çí„ÉÜ„Ç≠„Çπ„ÉàÂåñ
            transcript_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in utterances
            ])

            prompt = f"""‰ª•‰∏ã„ÅÆ„Ç§„É≥„Çø„Éì„É•„Éº„ÅÆ‰ºöË©±„ÇíË¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

# ‰ºöË©±ÂÜÖÂÆπ
{transcript_text}

Ë¶ÅÁ¥Ñ„ÅÆ„É´„Éº„É´:
- 3-5ÊñáÁ®ãÂ∫¶„ÅßÁ∞°ÊΩî„Å´
- Ë©±„Åï„Çå„Åü‰∏ªË¶Å„Å™„Éà„Éî„ÉÉ„ÇØ„ÇíÂê´„ÇÅ„Çã
- Ë©±ËÄÖÂêç„ÇíÂê´„ÇÅ„Å¶„Äå„Äá„Äá„Åï„Çì„ÅØ...„Äç„Å®„ÅÑ„ÅÜÂΩ¢Âºè„Åß

Ë¶ÅÁ¥Ñ:
"""

            response = await self.model.generate_content_async(prompt)
            summary = response.text.strip()

            logger.info(f"üìù Generated summary: {len(summary)} chars")
            return summary

        except Exception as e:
            logger.error(f"Failed to summarize transcript: {e}")
            return None

    async def generate_final_summary(
        self,
        front_summary: str,
        recent_transcript: List[Utterance]
    ) -> Optional[str]:
        """
        ÊúÄÁµÇË¶ÅÁ¥Ñ„ÇíÁîüÊàê

        Args:
            front_summary: ÂâçÂçä„ÅÆË¶ÅÁ¥Ñ
            recent_transcript: Áõ¥Ëøë„ÅÆÁô∫Ë©±

        Returns:
            ÊúÄÁµÇË¶ÅÁ¥Ñ or None
        """
        if not self.enabled:
            return None

        try:
            recent_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in recent_transcript
            ])

            prompt = f"""„Ç§„É≥„Çø„Éì„É•„ÉºÂÖ®‰Ωì„ÅÆÊúÄÁµÇË¶ÅÁ¥Ñ„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

# ÂâçÂçä„ÅÆË¶ÅÁ¥Ñ
{front_summary if front_summary else "Ôºà„Å™„ÅóÔºâ"}

# ÂæåÂçä„ÅÆ‰ºöË©±
{recent_text}

ÊúÄÁµÇË¶ÅÁ¥Ñ:
- 5-10ÊñáÁ®ãÂ∫¶
- „Ç§„É≥„Çø„Éì„É•„ÉºÂÖ®‰Ωì„ÅÆÊµÅ„Çå„ÇíÊääÊè°„Åß„Åç„Çã„Çà„ÅÜ„Å´
- ÈáçË¶Å„Å™„Éù„Ç§„É≥„Éà„ÇíÁÆáÊù°Êõ∏„Åç„ÅßÂê´„ÇÅ„Çã
"""

            response = await self.model.generate_content_async(prompt)
            summary = response.text.strip()

            logger.info(f"üìÑ Generated final summary: {len(summary)} chars")
            return summary

        except Exception as e:
            logger.error(f"Failed to generate final summary: {e}")
            return None

    async def improve_selected_text(
        self,
        selected_text: str,
        instruction: str,
        context: str = ""
    ) -> Optional[str]:
        """
        ÈÅ∏ÊäûÁØÑÂõ≤„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊîπÂñÑ

        Args:
            selected_text: ÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà
            instruction: ÊîπÂñÑÊåáÁ§∫Ôºà"„Éñ„É©„ÉÉ„Ç∑„É•„Ç¢„ÉÉ„Éó", "Êõ∏„ÅçÁõ¥„Åó", „Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„ÉàÔºâ
            context: ÂâçÂæå„ÅÆÊñáËÑà

        Returns:
            ÊîπÂñÑ„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà or None
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""„ÅÇ„Å™„Åü„ÅØ„Éó„É≠„ÅÆ„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„ÇíÊîπÂñÑ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

# ÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà
{selected_text}

# ÂâçÂæå„ÅÆÊñáËÑàÔºàÂèÇËÄÉÔºâ
{context if context else "ÔºàÊñáËÑà„Å™„ÅóÔºâ"}

# ÊåáÁ§∫
{instruction}

# ÈáçË¶Å„Å™Ê≥®ÊÑè‰∫ãÈ†Ö
- ÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„ÅÆÈÉ®ÂàÜ„Å†„Åë„ÇíÊîπÂñÑ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- „Ç≥„Éº„Éâ„Éï„Çß„É≥„ÇπÔºà```markdown „Å™„Å©Ôºâ„ÅØ‰Ωø„Çè„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
- Á¥îÁ≤ã„Å™ÊîπÂñÑÂæå„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ

ÊîπÂñÑÂæå„ÅÆ„ÉÜ„Ç≠„Çπ„Éà:
"""

            response = await self.model.generate_content_async(prompt)
            improved = response.text.strip()

            # „Ç≥„Éº„Éâ„Éï„Çß„É≥„Çπ„ÇíÂâäÈô§
            if improved.startswith('```markdown'):
                improved = improved[len('```markdown'):].strip()
            if improved.startswith('```'):
                improved = improved[3:].strip()
            if improved.endswith('```'):
                improved = improved[:-3].strip()

            logger.info(f"‚ú® Improved text: {len(improved)} chars")
            return improved

        except Exception as e:
            logger.error(f"Failed to improve text: {e}")
            return None

    async def restructure_as_subsection(
        self,
        selected_text: str,
        full_article: str
    ) -> Optional[str]:
        """
        ÈÅ∏ÊäûÁØÑÂõ≤„Çí1„Å§„ÅÆÂ∞èË¶ãÂá∫„ÅóÔºà##Ôºâ„Çª„ÇØ„Ç∑„Éß„É≥„Å´ÂÜçÊßãÊàê

        Args:
            selected_text: ÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà
            full_article: Ë®ò‰∫ãÂÖ®‰Ωì

        Returns:
            ÂÜçÊßãÊàê„Åï„Çå„Åü„Çª„ÇØ„Ç∑„Éß„É≥Ôºà## Ë¶ãÂá∫„Åó‰ªò„ÅçÔºâor None
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""„ÅÇ„Å™„Åü„ÅØ„Éó„É≠„ÅÆ„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅ1„Å§„ÅÆ„Åæ„Å®„Åæ„Å£„ÅüÂ∞èË¶ãÂá∫„Åó„Çª„ÇØ„Ç∑„Éß„É≥„Å´ÂÜçÊßãÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

# Ë®ò‰∫ãÂÖ®‰ΩìÔºàÂèÇËÄÉÔºâ
{full_article}

# ÈÅ∏ÊäûÁØÑÂõ≤Ôºà„Åì„ÅÆÈÉ®ÂàÜ„ÇíÂÜçÊßãÊàêÔºâ
{selected_text}

# ÊåáÁ§∫
1. ÈÅ∏ÊäûÁØÑÂõ≤„ÅÆÂÜÖÂÆπ„ÇíÂàÜÊûê„Åó„ÄÅÈÅ©Âàá„Å™Â∞èË¶ãÂá∫„ÅóÔºà##Ôºâ„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. Êï£„Çâ„Å∞„Å£„ÅüÂÜÖÂÆπ„Çí„ÄÅ1„Å§„ÅÆ„Åæ„Å®„Åæ„Å£„ÅüÊñáÁ´†„Å´ÂÜçÊßãÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ
3. Ë®ò‰∫ã„ÅÆ„Éà„Éº„É≥„Å´Âêà„Çè„Åõ„Å¶„Åè„Å†„Åï„ÅÑ
4. MarkdownÂΩ¢Âºè„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
5. „Ç≥„Éº„Éâ„Éï„Çß„É≥„ÇπÔºà```markdownÔºâ„ÅØ‰Ωø„Çè„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ

ÂÜçÊßãÊàê„Åï„Çå„Åü„Çª„ÇØ„Ç∑„Éß„É≥:
"""

            response = await self.model.generate_content_async(prompt)
            section = response.text.strip()

            # „Ç≥„Éº„Éâ„Éï„Çß„É≥„Çπ„ÇíÂâäÈô§
            if section.startswith('```markdown'):
                section = section[len('```markdown'):].strip()
            if section.startswith('```'):
                section = section[3:].strip()
            if section.endswith('```'):
                section = section[:-3].strip()

            logger.info(f"üì¶ Restructured subsection: {len(section)} chars")
            return section

        except Exception as e:
            logger.error(f"Failed to restructure subsection: {e}")
            return None

    async def restructure_as_section(
        self,
        selected_text: str,
        full_article: str
    ) -> Optional[str]:
        """
        ÈÅ∏ÊäûÁØÑÂõ≤„Çí1„Å§„ÅÆÂ§ßË¶ãÂá∫„ÅóÔºà#Ôºâ„Çª„ÇØ„Ç∑„Éß„É≥„Å´ÂÜçÊßãÊàê

        Args:
            selected_text: ÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà
            full_article: Ë®ò‰∫ãÂÖ®‰Ωì

        Returns:
            ÂÜçÊßãÊàê„Åï„Çå„Åü„Çª„ÇØ„Ç∑„Éß„É≥Ôºà# Ë¶ãÂá∫„Åó‰ªò„ÅçÔºâor None
        """
        if not self.enabled:
            return None

        try:
            prompt = f"""„ÅÇ„Å™„Åü„ÅØ„Éó„É≠„ÅÆ„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„Çí„ÄÅ1„Å§„ÅÆ„Åæ„Å®„Åæ„Å£„ÅüÂ§ßË¶ãÂá∫„Åó„Çª„ÇØ„Ç∑„Éß„É≥„Å´ÂÜçÊßãÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

# Ë®ò‰∫ãÂÖ®‰ΩìÔºàÂèÇËÄÉÔºâ
{full_article}

# ÈÅ∏ÊäûÁØÑÂõ≤Ôºà„Åì„ÅÆÈÉ®ÂàÜ„ÇíÂÜçÊßãÊàêÔºâ
{selected_text}

# ÊåáÁ§∫
1. ÈÅ∏ÊäûÁØÑÂõ≤„ÅÆÂÜÖÂÆπ„ÇíÂàÜÊûê„Åó„ÄÅÈÅ©Âàá„Å™Â§ßË¶ãÂá∫„ÅóÔºà#Ôºâ„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. „Çà„ÇäÂ§ß„Åç„Å™„ÉÅ„É£„É≥„ÇØ„Å®„Åó„Å¶„ÄÅË§áÊï∞„ÅÆÂ∞èË¶ãÂá∫„ÅóÔºà##Ôºâ„ÇíÂê´„ÇÄÊßãÈÄ†ÁöÑ„Å™„Çª„ÇØ„Ç∑„Éß„É≥„Å´ÂÜçÊßãÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ
3. Êï£„Çâ„Å∞„Å£„ÅüÂÜÖÂÆπ„Çí„ÄÅË´ñÁêÜÁöÑ„Å™ÊµÅ„Çå„ÇíÊåÅ„Å§„Åæ„Å®„Åæ„Å£„ÅüÊñáÁ´†„Å´ÂÜçÊßãÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ
4. Ë®ò‰∫ã„ÅÆ„Éà„Éº„É≥„Å´Âêà„Çè„Åõ„Å¶„Åè„Å†„Åï„ÅÑ
5. MarkdownÂΩ¢Âºè„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ
6. „Ç≥„Éº„Éâ„Éï„Çß„É≥„ÇπÔºà```markdownÔºâ„ÅØ‰Ωø„Çè„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ

ÂÜçÊßãÊàê„Åï„Çå„Åü„Çª„ÇØ„Ç∑„Éß„É≥:
"""

            response = await self.model.generate_content_async(prompt)
            section = response.text.strip()

            # „Ç≥„Éº„Éâ„Éï„Çß„É≥„Çπ„ÇíÂâäÈô§
            if section.startswith('```markdown'):
                section = section[len('```markdown'):].strip()
            if section.startswith('```'):
                section = section[3:].strip()
            if section.endswith('```'):
                section = section[:-3].strip()

            logger.info(f"üì¶ Restructured major section: {len(section)} chars")
            return section

        except Exception as e:
            logger.error(f"Failed to restructure section: {e}")
            return None

    async def generate_article_section(
        self,
        current_article: str,
        recent_transcript: List[Utterance],
        front_summary: str = ""
    ) -> Optional[str]:
        """
        ÊñáÂ≠óËµ∑„Åì„Åó„Åã„ÇâË®ò‰∫ã„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÁîüÊàê„ÉªËøΩË®ò

        Args:
            current_article: ÁèæÂú®„ÅÆÂéüÁ®øÂÜÖÂÆπ
            recent_transcript: Áõ¥Ëøë„ÅÆÁô∫Ë©±„É™„Çπ„ÉàÔºàÊúÄÊñ∞5Áô∫Ë©±Ôºâ
            front_summary: „Åì„Çå„Åæ„Åß„ÅÆË¶ÅÁ¥ÑÔºàÊñáËÑàË£úÂº∑Áî®Ôºâ

        Returns:
            ËøΩÂä†„Åô„Åπ„ÅçË®ò‰∫ã„Çª„ÇØ„Ç∑„Éß„É≥ or None
        """
        if not self.enabled:
            return None

        try:
            # Áõ¥Ëøë„ÅÆÁô∫Ë©±„Çí„ÉÜ„Ç≠„Çπ„ÉàÂåñ
            recent_text = "\n".join([
                f"{u.speaker_name}: {u.text}"
                for u in recent_transcript
            ])


            prompt = f"""„ÅÇ„Å™„Åü„ÅØ„Ç§„É≥„Çø„Éì„É•„ÉºË®ò‰∫ã„ÅÆ„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇ
ÊñáÂ≠óËµ∑„Åì„Åó„ÇíÂÖÉ„Å´„ÄÅË®ò‰∫ã„ÅÆ‰∏ÄÈÉ®„Çí**MarkdownÂΩ¢Âºè**„ÅßÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

# ÁèæÂú®„ÅÆË®ò‰∫ãÂÜÖÂÆπ
{current_article if current_article else "Ôºà„Åæ„Å†Ë®ò‰∫ã„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇÊúÄÂàù„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÔºâ"}

# „Åì„Çå„Åæ„Åß„ÅÆË¶ÅÁ¥ÑÔºàÂèÇËÄÉÔºâ
{front_summary if front_summary else "ÔºàË¶ÅÁ¥Ñ„Å™„ÅóÔºâ"}

# Áõ¥Ëøë„ÅÆ‰ºöË©±ÔºàÊñáÂ≠óËµ∑„Åì„ÅóÔºâ
{recent_text}

# ÊåáÁ§∫
1. **ÂøÖ„Åö `##` „ÅßÂßã„Åæ„ÇãÂ∞èË¶ãÂá∫„Åó„Çí‰ªò„Åë„Å¶„Åè„Å†„Åï„ÅÑ**Ôºà‰æã: `## AI„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆÂèØËÉΩÊÄß`Ôºâ
2. Â∞èË¶ãÂá∫„Åó„ÅÆÂæå„Å´„ÄÅ‰ºöË©±ÂÜÖÂÆπ„ÇíËá™ÁÑ∂„Å™ÊñáÁ´†„Å´Â§âÊèõ„Åó„ÅüÊú¨Êñá„ÇíÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ
3. „Ç§„É≥„Çø„Éì„É•„Ç§„Éº„ÅÆÁô∫Ë®Ä„ÅØ„Åù„ÅÆ„Åæ„ÅæÂºïÁî®„Åó„ÄÅË®ò‰∫ã„Å®„Åó„Å¶Ë™≠„Åø„ÇÑ„Åô„Åè„Åó„Å¶„Åè„Å†„Åï„ÅÑ
4. Êó¢Â≠ò„ÅÆË®ò‰∫ãÂÜÖÂÆπ„ÇíÁπ∞„ÇäËøî„Åï„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
5. 150-300ÊñáÂ≠óÁ®ãÂ∫¶„ÅÆÁü≠„ÅÑ„Çª„ÇØ„Ç∑„Éß„É≥„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ
6. **Áµ∂ÂØæ„Å´** ```markdown „ÅÆ„Çà„ÅÜ„Å™„Ç≥„Éº„Éâ„Éï„Çß„É≥„Çπ„ÅØ‰Ωø„Çè„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ
7. **Áµ∂ÂØæ„Å´**„Çø„Ç§„É†„Çπ„Çø„É≥„Éó„ÇÑË©±ËÄÖÂêçÔºà[19:23:55] Interviewer:Ôºâ„ÇíÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ

Âá∫ÂäõÂΩ¢ÂºèÔºàÂøÖ„ÅöÂÆà„Å£„Å¶„Åè„Å†„Åï„ÅÑÔºâ:
## Â∞èË¶ãÂá∫„Åó

Êú¨ÊñáÔºàËá™ÁÑ∂„Å™Ë®ò‰∫ãÊñá‰Ωì„ÅßÔºâ
"""

            response = await self.model.generate_content_async(prompt)
            section = response.text.strip()

            # „Ç≥„Éº„Éâ„Éï„Çß„É≥„Çπ„ÇíÂâäÈô§ÔºàÂøµ„ÅÆ„Åü„ÇÅÔºâ
            if section.startswith('```markdown'):
                section = section[len('```markdown'):].strip()
            if section.startswith('```'):
                section = section[3:].strip()
            if section.endswith('```'):
                section = section[:-3].strip()

            # „Çø„Ç§„É†„Çπ„Çø„É≥„Éó‰ªò„ÅçÁô∫Ë©±Ë°å„ÇíÂâäÈô§Ôºà[HH:MM:SS] Speaker:Ôºâ
            lines = section.split('\n')
            cleaned_lines = []
            for line in lines:
                # [19:23:55] Interviewer: „ÅÆ„Çà„ÅÜ„Å™„Éë„Çø„Éº„É≥„Çí„Çπ„Ç≠„ÉÉ„Éó
                if re.match(r'^\[\d{2}:\d{2}:\d{2}\]\s+\w+:\s*', line):
                    continue
                cleaned_lines.append(line)
            section = '\n'.join(cleaned_lines).strip()

            logger.info(f"üìù Generated article section: {len(section)} chars")
            return section

        except Exception as e:
            logger.error(f"Failed to generate article section: {e}")
            return None

    async def generate_text(self, system_prompt: str, user_prompt: str) -> Optional[str]:
        # ... (docstring) ...
        if not self.enabled:
            print("DEBUG: generate_text - Client disabled (gemini)")
            return None

        try:
            print(f"DEBUG: generate_text - Start. Model: {self.model.model_name}")
            full_prompt = f"{system_prompt}\n\n---\n\n{user_prompt}"
            response = await self.model.generate_content_async(full_prompt, safety_settings=self.safety_settings)
            
            print(f"DEBUG: generate_text - Response received. Candidates: {len(response.candidates)}")
            
            if response.prompt_feedback and response.prompt_feedback.block_reason:
                print(f"DEBUG: generate_text - Blocked: {response.prompt_feedback}")
                logger.warning(f"‚ö†Ô∏è Prompt blocked: {response.prompt_feedback}")
                return None
            return response.text.strip()
        except Exception as e:
            print(f"DEBUG: generate_text - Exception: {e}")
            logger.error(f"Failed to generate text (Gemini): {e}")
            self.last_error = str(e)
            return None


# „Ç∞„É≠„Éº„Éê„É´„Ç§„É≥„Çπ„Çø„É≥„Çπ
gemini_client = GeminiClient()
